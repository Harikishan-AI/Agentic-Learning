{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07727593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.memory import ListMemory, MemoryContent, MemoryMimeType\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fbc173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "api_key = os.getenv(\"OPEN_ROUTER_API_KEY\") \n",
    "\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "base_url=\"https://openrouter.ai/api/v1\",\n",
    "api_key=api_key,\n",
    "model=\"qwen/qwen3-vl-30b-a3b-thinking\",    \n",
    "model_info={\n",
    "    \"family\": \"qwen\",\n",
    "    \"vision\": False,\n",
    "    \"function_calling\": True,\n",
    "    \"json_output\": True,\n",
    "    \"structured_output\": True\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb340b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "userMemory = ListMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a306502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "await userMemory.add(MemoryContent(\n",
    "    content='The weather unit should be degree celsius (metric)',\n",
    "    mime_type=MemoryMimeType.TEXT\n",
    "))\n",
    "\n",
    "await userMemory.add(MemoryContent(\n",
    "    content='The User is Vegetarian',\n",
    "    mime_type=MemoryMimeType.TEXT\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff61ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_weather(city: str, units: str = \"imperial\") -> str:\n",
    "    if units == \"imperial\":\n",
    "        return f\"The weather in {city} is 73 °F and Sunny.\"\n",
    "    elif units == \"metric\":\n",
    "        return f\"The weather in {city} is 23 °C and Sunny.\"\n",
    "    else:\n",
    "        return f\"Sorry, I don't know the weather in {city}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a5f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_agent = AssistantAgent(\n",
    "    'weather_assistant',\n",
    "    model_client=model_client,\n",
    "    tools=[get_weather],\n",
    "    memory=[userMemory]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be8f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "What is the weather in Delhi ?\n",
      "---------- MemoryQueryEvent (weather_assistant) ----------\n",
      "[MemoryContent(content='The weather unit should be degree celsius (metric)', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None), MemoryContent(content='The User is Vegetarian', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None)]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Multiple and Not continuous system messages are not supported if model_info['multiple_system_messages'] is False",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m stream = assistant_agent.run_stream(task=\u001b[33m'\u001b[39m\u001b[33mWhat is the weather in Delhi ?\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Console(stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\projects\\Agentic-Learning\\.venv\\Lib\\site-packages\\autogen_agentchat\\ui\\_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\projects\\Agentic-Learning\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_base_chat_agent.py:202\u001b[39m, in \u001b[36mBaseChatAgent.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token, output_task_messages)\u001b[39m\n\u001b[32m    200\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    201\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid message type in sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(msg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_messages_stream(input_messages, cancellation_token):\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, Response):\n\u001b[32m    204\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m message.chat_message\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\projects\\Agentic-Learning\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:953\u001b[39m, in \u001b[36mAssistantAgent.on_messages_stream\u001b[39m\u001b[34m(self, messages, cancellation_token)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# STEP 4: Run the first inference\u001b[39;00m\n\u001b[32m    952\u001b[39m model_result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m inference_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_llm(\n\u001b[32m    954\u001b[39m     model_client=model_client,\n\u001b[32m    955\u001b[39m     model_client_stream=model_client_stream,\n\u001b[32m    956\u001b[39m     system_messages=system_messages,\n\u001b[32m    957\u001b[39m     model_context=model_context,\n\u001b[32m    958\u001b[39m     workbench=workbench,\n\u001b[32m    959\u001b[39m     handoff_tools=handoff_tools,\n\u001b[32m    960\u001b[39m     agent_name=agent_name,\n\u001b[32m    961\u001b[39m     cancellation_token=cancellation_token,\n\u001b[32m    962\u001b[39m     output_content_type=output_content_type,\n\u001b[32m    963\u001b[39m     message_id=message_id,\n\u001b[32m    964\u001b[39m ):\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inference_output, CreateResult):\n\u001b[32m    966\u001b[39m         model_result = inference_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\projects\\Agentic-Learning\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:1109\u001b[39m, in \u001b[36mAssistantAgent._call_llm\u001b[39m\u001b[34m(cls, model_client, model_client_stream, system_messages, model_context, workbench, handoff_tools, agent_name, cancellation_token, output_content_type, message_id)\u001b[39m\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m model_result\n\u001b[32m   1108\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1109\u001b[39m     model_result = \u001b[38;5;28;01mawait\u001b[39;00m model_client.create(\n\u001b[32m   1110\u001b[39m         llm_messages,\n\u001b[32m   1111\u001b[39m         tools=tools,\n\u001b[32m   1112\u001b[39m         cancellation_token=cancellation_token,\n\u001b[32m   1113\u001b[39m         json_output=output_content_type,\n\u001b[32m   1114\u001b[39m     )\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m model_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\projects\\Agentic-Learning\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:673\u001b[39m, in \u001b[36mBaseOpenAIChatCompletionClient.create\u001b[39m\u001b[34m(self, messages, tools, tool_choice, json_output, extra_create_args, cancellation_token)\u001b[39m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    664\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    665\u001b[39m     messages: Sequence[LLMMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    671\u001b[39m     cancellation_token: Optional[CancellationToken] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    672\u001b[39m ) -> CreateResult:\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     create_params = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_create_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_create_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    680\u001b[39m     future: Union[Task[ParsedChatCompletion[BaseModel]], Task[ChatCompletion]]\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m create_params.response_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    682\u001b[39m         \u001b[38;5;66;03m# Use beta client if response_format is not None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\projects\\Agentic-Learning\\.venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:597\u001b[39m, in \u001b[36mBaseOpenAIChatCompletionClient._process_create_args\u001b[39m\u001b[34m(self, messages, tools, tool_choice, json_output, extra_create_args)\u001b[39m\n\u001b[32m    593\u001b[39m     _first_system_message_idx = idx\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m _last_system_message_idx + \u001b[32m1\u001b[39m != idx:\n\u001b[32m    595\u001b[39m     \u001b[38;5;66;03m# That case, system message is not continuous\u001b[39;00m\n\u001b[32m    596\u001b[39m     \u001b[38;5;66;03m# Merge system messages only contiues system messages\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    598\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMultiple and Not continuous system messages are not supported if model_info[\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmultiple_system_messages\u001b[39m\u001b[33m'\u001b[39m\u001b[33m] is False\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    599\u001b[39m     )\n\u001b[32m    600\u001b[39m system_message_content += message.content + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    601\u001b[39m _last_system_message_idx = idx\n",
      "\u001b[31mValueError\u001b[39m: Multiple and Not continuous system messages are not supported if model_info['multiple_system_messages'] is False"
     ]
    }
   ],
   "source": [
    "stream = assistant_agent.run_stream(task='What is the weather in Delhi ?')\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294fba14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
